{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5:  Unsupervised Learning \n",
    "\n",
    "Outline:\n",
    "1. Preamble: unserializing data\n",
    "2. Non-Negative Matrix Factorization \n",
    "3. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lab we will focus on unsupervised learning, a set of statistical tools intended for the setting in which we have only a set of features $X_{1},\\ldots,X_{p}$ measured on $n$ observations. \n",
    "\n",
    "We are not interested in prediction, because we do not have an associated response variable $Y$ (as we did in Lab4). Rather, the goal is to discover interesting things about the measurements on $X_{1},\\ldots,X_{p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preamble: unserializing data\n",
    "\n",
    "`pickle` file is a python module for saving data objects into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess as sp\n",
    "import pickle # to serialize/unserialize python data objects\n",
    "\n",
    "import helper_basketball as h\n",
    "import imp\n",
    "imp.reload(h);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our original data set:\n",
    "allshots = pickle.load(open('allshots2016-17.pkl', 'rb'))\n",
    "allmade = allshots\n",
    "allmade[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In computer science, in the context of data storage, serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted (for example, across a network connection link) and reconstructed later (possibly in a different computer environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bin edge definitions in inches\n",
    "xedges = (np.linspace(start=-25, stop=25, num=151, dtype=np.float)) * 12\n",
    "yedges = (np.linspace(start= -4, stop=31, num=106, dtype=np.float)) * 12\n",
    "\n",
    "## 2d histogram containers for binned counts and smoothed binned counts\n",
    "all_counts = {}\n",
    "all_smooth = {}\n",
    "\n",
    "## data matrix: players (row) by vectorized 2-d court locations (column)\n",
    "for i, one in enumerate(allmade.groupby('PlayerID')):\n",
    "    \n",
    "    ## what does this line do?\n",
    "    pid, pdf = one\n",
    "    \n",
    "    ## h.bin_shots: what is this function doing?\n",
    "    tmp1, xedges, yedges = h.bin_shots(pdf, bin_edges=(xedges, yedges), density=True, sigma=2)\n",
    "    tmp2, xedges, yedges = h.bin_shots(pdf, bin_edges=(xedges, yedges), density=False)\n",
    "    \n",
    "    ## vectorize and store into dictionary\n",
    "    # see: https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape\n",
    "    all_smooth[pid] = tmp1.reshape(-1)\n",
    "    all_counts[pid] = tmp2.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we proceed to serialize the dictionary of smoothed patters (first part of Hw3). \n",
    "- In this case, the object `all_smooth` is a dictionary that consists of `len(all_smooth)` (362) arrays of length `15750`. \n",
    "- Each entry in `all_smooth` represents the smoothed frequency of shots along the bins generated in the code above for a given player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(np.stack(all_smooth.values()).T, open('allpatterns2016-17.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unserialize smothed frequency values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pickle.load(open('allpatterns2016-17.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Non-negative Matrix Factorization (NMF) \n",
    "\n",
    "Non-negative matrix factorization was used on the smoothed shooting pattern data of around 360 players. The result was useful in\n",
    "* Bases: Identifying modes of shooting style (number of modes was determined by `n_components` argument to `NMF` function)\n",
    "* Coefficients: How each players shooting style could be expressed as a linear combination of these bases (matrix multiplication between the bases and coefficients achieve this)\n",
    "\n",
    "Recall the following. Given some matrix $X$ is $p\\times n$ matrix, NMF computes the following factorization:\n",
    "$$ \\min_{W,H} \\| X - WH \\|_F\\\\\n",
    "\\text{ subject to } W\\geq 0,\\ H\\geq 0, $$\n",
    "where $W$ is ${p\\times r}$ matrix and $H$ is ${r\\times n}$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non-negative Matrix Factorization\n",
    "def non_negative_marix_decomp(n_components,train_data):\n",
    "    import sklearn.decomposition as skld\n",
    "    model = skld.NMF(n_components=n_components, init='nndsvda', max_iter=500, random_state=0)\n",
    "    W = model.fit_transform(train_data)\n",
    "    H = model.components_\n",
    "    nmf = (W,H)\n",
    "    return(nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 3\n",
    "# X = np.stack(all_smooth.values()).T # Why did we transpose it?\n",
    "W_3,H_3 = non_negative_marix_decomp(n_components = r,train_data = X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-constructing shooting pattern for *one* player\n",
    "\n",
    "Below we re-construct the shooting pattern for a single player. By \"reconstructing\" we mean use the approximation $$\\hat{X} = WH$$ obtained via NMF.\n",
    "\n",
    "**Remark**: In hw3 you have to plot the shooting pattern reconstruction for *at least two players* and $r\\in \\{3,20\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_hat = np.matmul(W_3,H_3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20,60))\n",
    "\n",
    "# I took the first player appearing in first column \n",
    "# (you probably want to do more interesting players)\n",
    "h.plot_shotchart(X[:,1], xedges, yedges, ax=ax[0]) \n",
    "h.plot_shotchart(X3_hat[:,1], xedges, yedges, ax=ax[1])\n",
    "ax[0].set_title('Original Shooting Pattern')\n",
    "ax[1].set_title('Estimated Shooting Pattern (r=3)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the Frobenious norm between the actual and estimated shooting patterns for several values of number of bases, $r = 1,\\ldots,10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frobenious norm\n",
    "from numpy import linalg as LA # To compute Frobenious norm\n",
    "LA.norm(X-np.matmul(W_3,H_3),'fro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA # To compute Frobenious norm\n",
    "r_values = np.arange(10)+1\n",
    "error_norm = []\n",
    "for r in r_values:\n",
    "    W,H = non_negative_marix_decomp(n_components = r,train_data = X)\n",
    "    error_norm.append(LA.norm(X-np.matmul(W,H),'fro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r_values,error_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "A = np.array([[0.1, .32, .2, 0.4, 0.8], \n",
    "             [.23, .18, .56, .61, .12], \n",
    "             [.9, .3, .6, .5, .3],\n",
    "             [.34, .75, .91, .19, .21]])\n",
    "A.shape # since number of column is 5, we want a correlation matrix with 5*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_A = np.corrcoef(A,rowvar=0)\n",
    "Corr_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix of frequency data\n",
    "\n",
    "For computational purposes, let us consider only the first 100 columns of our information matrix $X$. We will then compute its correlation matrix and plot it as a heatmap using the `seaborn` package.\n",
    "\n",
    "**Remark**: In hw3 you have to consider all 362 players. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X[:,0:99].copy()\n",
    "R = np.corrcoef(X_test,rowvar=0) # Transpose to compute correlation between colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "ax = sns.heatmap(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hierarchical Clustering\n",
    "\n",
    "\n",
    "*Clustering* refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. \n",
    "\n",
    "- Hierarchical Clustering: the number of groups or clusters need not to be specified in advanced (in contrast to $K$-means).\n",
    "- Bottom-up or agglomerative clustering: Each observation is assumed to belong to its own cluster and they are sequentially fused based on a measure of dissimilartiy between clusters (also known as linkage). \n",
    "- The relationship among different clusters is represented graphically via a tree-like plot known as *dendogram*.\n",
    "\n",
    "The hierarchical clustering dendrogram is obtained via an extremely simple algorithm:\n",
    "\n",
    "Starting out at the bottom of the dendrogram, each of the $n$ observations is treated as its own cluster. The two clusters that are most similar to each other are then fused so that there now are $n - 1$ clusters. Next the two clusters that are most similar to each other are fused again, so that there now are $n - 2$ clusters. The algorithm proceeds in this fashion until all of the observations belong to one single cluster, and the dendrogram is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Complete linkage**: Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(X_test.T,method='complete',metric='correlation') # why did we transpose X.test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the dendogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the cluster *id* for each player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "no_max_clust = 10\n",
    "cluster_id = fcluster(Z,no_max_clust,criterion='maxclust')\n",
    "cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of correlation matrix in terms of cluster assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_clust = np.corrcoef(X_test.T[np.argsort(cluster_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(R_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
