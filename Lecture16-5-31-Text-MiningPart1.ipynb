{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 14 - Text Mining using Yelp Data\n",
    "# 1. Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Dataset Challenge \n",
    "\n",
    "\n",
    "Yelp challenges students to use their data in innovative ways and break ground in research. There is a myriad of deep, machine learning questions to tackle with this rich dataset:\n",
    "\n",
    "- How well can you guess a review's rating from its text alone? \n",
    "- Can you take all of the reviews of a business and predict when it will be the most busy, or when the business is open? \n",
    "- Can you predict if a business is good for kids? Has WiFi? Has Parking? \n",
    "- What makes a review useful, funny, or cool? \n",
    "- Can you figure out which business a user is likely to review next? \n",
    "- How much of a business's success is really just location, location, location? \n",
    "- What businesses deserve their own subcategory (i.e., Szechuan or Hunan versus just “Chinese restaurants”), and can you learn this from the review text?\n",
    "- What are the differences between the cities in the dataset?\n",
    "\n",
    "See some of the [past winners](https://www.yelp.com/dataset/challenge/winners) and hundreds of [academic papers](https://scholar.google.com/scholar?q=citation%3A+Yelp+Dataset&btnG=&hl=en&as_sdt=0%2C5) written using the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Yelp Dataset JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data: https://www.yelp.com/dataset/download. \n",
    "\n",
    "Each file is composed of a single object type, one JSON-object per-line.\n",
    "\n",
    "- `business.json`: contains business data including location data, attributes, and categories.\n",
    "- `review.json`: Contains full review text data including the `user_id` that wrote the review and the `business_id` the review is written for.\n",
    "- `user.json`: User data including the user's friend mapping and all the metadata associated with the user.\n",
    "- `checkin.json`: Checkins on a business.\n",
    "- `tip.json`: Tips written by a user on a business. Tips are shorter than reviews and tend to convey quick suggestions.\n",
    "- `photos` (from the photos auxiliary file): This file is formatted as a JSON list of objects.\n",
    "\n",
    "Take a look at some examples to get you started: https://github.com/Yelp/dataset-examples.\n",
    "\n",
    "The dataset consist of very large json files. Here are the file sizes:\n",
    "\n",
    "\n",
    "| File name                               | Size |\n",
    "|-----------------------------------------|------|\n",
    "| business.json                           | 139M |\n",
    "| checkin.json                            |  61M |\n",
    "| Dataset_Challenge_Dataset_Agreement.pdf |  98K |\n",
    "| photos.json                             |  26M |\n",
    "| review.json                             | 4.0G |\n",
    "| tip.json                                | 189M |\n",
    "| user.json                               | 1.8G |\n",
    "| Yelp_Dataset_Challenge_Round_11.pdf     | 111K |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data loading\n",
    "\n",
    "\n",
    "## Business data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading `business.json` in order to filter out business only with a sufficiently large number of reviews.\n",
    "\n",
    "The file `business.json` was previously saved in a Google drive so that we can `wget` from the shared link. The latter process is done using the bash [script below](https://gist.github.com/iamtekeste/3cdfd0366ebfd2c0d805#gistcomment-2359248):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "function gdrive_download () {\n",
    "  CONFIRM=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \"https://drive.google.com/uc?export=download&id=$1\" -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\n",
    "  wget --quiet --load-cookies /tmp/cookies.txt \"https://drive.google.com/uc?export=download&confirm=$CONFIRM&id=$1\" -O $2\n",
    "  rm -rf /tmp/cookies.txt\n",
    "}\n",
    "# Fist argument is the file id (which we get from right-clicking the file in Google drive and\n",
    "# copying the last string appearing after id=)\n",
    "gdrive_download 1nmPJorJ4uGoHInq_1lz1ePElvtVjbA3l business.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n1 business.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `jq`\n",
    "\n",
    "To avoid reading `business.json` (a very large file) into memory in order to select businesses with high number of reviews, we take avantage of the fact that business information consists of several JSON objects and use a command line tool called [jq](https://stedolan.github.io/jq/) in order to slice, filter, map and transform JSON data with the same ease that `sed`, `awk`, `grep`.\n",
    "\n",
    "Also, `jq` is useful for processing large files using the [streaming parser](https://github.com/stedolan/jq/wiki/FAQ#streaming-json-parser). Streaming parser does not load the whole file into memory.\n",
    "\n",
    "You can find some `jq` basic examples [here](https://robots.thoughtbot.com/jq-is-sed-for-json). Another very useful resource to text `jq` commands on-line: https://jqplay.org/.\n",
    "\n",
    "The `jq` line below executes the following: \n",
    "\n",
    "1. [Selects a subset of keys from an object\n",
    "](https://stackoverflow.com/questions/29518137/jq-selecting-a-subset-of-keys-from-an-object?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa) using `with_entries()`. \n",
    "\n",
    "2. [Filters out the JSON lines according to a specific criteria](http://bigdatums.net/2016/11/16/filter-json-records-by-value-with-jq/) using `select()`.\n",
    "\n",
    "Futheremore, we use the option `jq --compact-output` (or `-c`) to get each object on a newline. \n",
    "\n",
    "Notice that in analogy to other bash commands, we can use the pipe operator `|` to use the output of one commands as the input of the subsequent one. \n",
    "\n",
    "The output is also re-directed  into `train_business.json` by using `>`.\n",
    "\n",
    "\n",
    "`business.json` contains the field `review_count` that  gives the number of reviews provided by Yelp users for any given business. We will start filtering out all businesses with at least **500 reviews** minimum number of reviews to conduct our analysis. We moreover select other fields of interest such as `business_id`, `state`, `city`, `categories` and `attributes`. \n",
    "\n",
    "Pretty-print first business with `jq`:\n",
    "\n",
    "**Please note that jp is only avaliable in the jupyter hub server, most laptop don't have it installed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n1 business.json | jq ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "A filter subsets attributes. Working with a few entries is simple with `head` and piping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n5 business.json \\\n",
    "    | jq 'with_entries(select([.key] | inside([\"business_id\", \"review_count\"])))'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also subset by values of `review_count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n5 business.json \\\n",
    "    | jq 'with_entries(select([.key] | inside([\"business_id\", \"review_count\"]))) \\\n",
    "          | select(.review_count>20)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can process the full data by making desired changes, and save to `train_business.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes some time to run\n",
    "! jq -c 'with_entries( \\\n",
    "            select([.key] | inside([\"business_id\",\"state\",\"city\",\"categories\",\"attributes\",\"review_count\"])) \\\n",
    "        ) | select(.review_count>500)' business.json > train_business.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not run\n",
    "! wc -l train_business.json # Number of businesses with at least 500 reviews (923)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now read `train_business.json` [into python](https://stackoverflow.com/questions/46790390/how-to-read-a-large-json-in-pandas), keeping in mind that the latter is a JSON Lines object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('train_business.json') as json_file:      \n",
    "    data_business = json_file.readlines()\n",
    "    data_business = list(map(json.loads, data_business)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our training data we will consider only 100 randomly chosen businesses from the subset of businesses with at least 500 reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_business = [data_business[index] for index \\\n",
    "                  in np.random.randint(0,len(data_business),size=100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review data\n",
    "\n",
    "We now collect the actual reviews corresponding to the 100 randomly chosen businesses with at least 500 reviews appearing in `train_business`. \n",
    "\n",
    "After trying doing so using `jq` (which would require to filter according to a list of `business_id`), I found that the most efficient way to do so is to read `review.json` (which contains ~5M JSON Lines) into python and then perform list comprehension to select the reviews for the businesses in question. The resulting *list* (named `train_review`) is serialized as a pkl object.\n",
    "\n",
    "The above procedure is included in `read_reviews.py`.\n",
    "\n",
    "Note that other approaches (including using python),\n",
    "* Read into python environment for processing\n",
    "* Use `jq` [stream parser](https://github.com/stedolan/jq/wiki/FAQ#streaming-json-parser)?\n",
    "* Use `grep` to search for business ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat read_reviews.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text Mining on Yelp Dataset\n",
    "\n",
    "The main resource that we will use for performing text mining on Yelp reviews text will be [Text Analytics with Python](https://github.com/dipanjanS/text-analytics-with-python).\n",
    "\n",
    "We will start by downloading the auxiliary functions  which are part of the textbook repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/dipanjanS/text-analytics-with-python/master/Chapter-4/feature_extractors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "function gdrive_download () {\n",
    "  CONFIRM=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \"https://drive.google.com/uc?export=download&id=$1\" -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\n",
    "  wget --load-cookies /tmp/cookies.txt \"https://drive.google.com/uc?export=download&confirm=$CONFIRM&id=$1\" -O $2\n",
    "  rm -rf /tmp/cookies.txt\n",
    "}\n",
    "\n",
    "gdrive_download 1fHyBolkVadlT3QB6C7pu9-qVrsWde7Tg train_review.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pickle\n",
    "train_review = pickle.load(open('train_review.pkl', \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_review[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text\n",
    "review_corpus = [text['text'] for text in train_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example (10 reviews)\n",
    "text = review_corpus[:10].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some of the main steps outline a typical workflow for for text mining, assuming we have our dataset already downloaded and ready to be used:\n",
    "\n",
    "1. Text normalization\n",
    "2. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization\n",
    "\n",
    "Text normalization is a process that consists of a series of steps that should be followed to wrangle, clean, and standardize textual data into a form that could be consumed by other NLP and analytics systems and applications as input. \n",
    "\n",
    "Tokenization is a process of segmenting text into meaningful units. For example, \n",
    "\n",
    "For O'Neill, which of the following is the desired tokenization: neill, oneill, o'neill, (o')(neill), (o)(neill)?\n",
    "\n",
    "And for aren't: aren't, arent, (are)(n't), (aren)(t)?\n",
    "\n",
    "Besides tokenization, other techniques include:\n",
    "\n",
    "1. Removing special characters (text cleaning)\n",
    "2. Case conversion\n",
    "3. Correcting spellings\n",
    "4. Removing stopwords and other unnecessary terms\n",
    "5. Lemmatization\n",
    "\n",
    "Text normalization is also often called *text cleansing or wrangling*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some packages can be installed by yourself\n",
    "# However, on course Jupyter notebook image, your packages will disappear if the instance is shutdown\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dictionaty of stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') # tockenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tokenizing Text\n",
    "\n",
    "\n",
    "Sentence tokenization is the process of splitting a text corpus into sentences that act as the first level of tokens which the corpus is comprised of. This is also known as sentence segmentation, because we try to segment the text into meaningful sentences. Any text corpus is a body of text where each paragraph comprises several sentences.\n",
    "\n",
    "We will use the `nltk` framework, which provides various interfaces for performing sentence tokenization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand contractions\n",
    "\n",
    "Contractions are shortened version of words or syllables. They exist in either written or spoken forms. Shortened versions of existing words are created by removing specific letters and sounds. In case of English contractions, they are often created by removing one of the vowels from the word. \n",
    "\n",
    "A vocabulary for contractions and their corresponding expanded forms that you can access in the file `contractions2.py` in a Python dictionary (which we again download from the textbook repo).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        # not sure why below is there\n",
    "        # expanded_contraction = first_char+expanded_contraction[1:] \n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contraction map is a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions2 import CONTRACTION_MAP\n",
    "CONTRACTION_MAP #in order to have this running, please run the cells below that says import Contraction_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_contractions(\"I could've gone, but didn't go\", CONTRACTION_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand contractions\n",
    "from contractions2 import CONTRACTION_MAP\n",
    "\n",
    "text = [expand_contractions(sentence, CONTRACTION_MAP) for sentence in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Special Characters\n",
    "\n",
    "One important task in text normalization involves removing unnecessary and special characters. These may be special symbols or even punctuation that occurs in sentences. This step is often performed before or after tokenization. The main reason for doing so is because often punctuation or special characters do not have much significance when we analyze the text and utilize it for extracting features or information based on NLP and ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [remove_special_characters(sentence) for sentence in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "Stopwords are words that have little or no significance. They are usually removed from text during processing so as to retain words having maximum significance and context. Stopwords are usually words that end up occurring the most if you aggregated any corpus of text based on singular tokens and checked their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text,stopword_list):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [remove_stopwords(sentence,\n",
    "                         stopword_list=list(ENGLISH_STOP_WORDS))\\\n",
    "        for sentence in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Parts of Speech and Lemmatization\n",
    "\n",
    "Now that we have a function for expanding contractions, we implement a function for standardizing our text data by bringing word tokens to their base or root form using lemmatization. For example, \n",
    "\n",
    "* better to good (lemmatization but not stemming would get correct)\n",
    "* walking to walk (lemmatization and stemming would both yield same result)\n",
    "* meeting? (part of speech is needed for correctly normalizing this text)\n",
    "\n",
    "The following functions will help us in achieving that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(['averaged_perceptron_tagger',\n",
    "               'universal_tagset',\n",
    "               'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog jumps'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens, tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    tagged_text = pos_tag(text)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "- The process of lemmatization is very similar to stemming; you remove word affixes to get to a base form of the word. \n",
    "\n",
    "- But in this case, this base form is also known as the root word, but not the root stem. \n",
    "\n",
    "- The difference is that the root stem may not always be a lexicographically correct word; that is, it may not be present in the dictionary. The root word, also known as the lemma, will always be present in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize text based on POS tags    \n",
    "def lemmatize_text(text):\n",
    "    text = tokenize_text(text)\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tokenize_text(sentence) for sentence in text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [tokenize_text(tokens) for tokens in text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization pipeline\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def normalize_text(text,tokenize=False):\n",
    "    text = expand_contractions(text, CONTRACTION_MAP)\n",
    "    text = lemmatize_text(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = text.lower()\n",
    "    text = remove_stopwords(text,ENGLISH_STOP_WORDS)\n",
    "    text = keep_text_characters(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_text(review_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(review_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time consuming (processed file is provided for download)\n",
    "# review_corpus_norm = [normalize_text(text) for text in review_corpus]\n",
    "# pickle.dump(review_corpus_norm,open('review_corpus_norm.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "- In ML terminology, features are unique, measurable attributes or properties for each observation or data point in a dataset. Features are usually numeric in nature and can be absolute numeric values or categorical.\n",
    "- With textual data there is the added challenge of figuring out how to transform textual data and extract numeric features from it.\n",
    "\n",
    "## Vector Space Model\n",
    "\n",
    "Say we have a document $D$ in a document vector space VS. The number of dimensions or columns for each document will be the total number of distinct terms or words for all documents in the vector space. So, the vector space can be denoted\n",
    "$$\n",
    "VS:=\\{W_{1},\\ldots,W_{n}\\}\n",
    "$$\n",
    "where there are $n$ distinct words across all documents. Now we can represent document $D$ in this vector space as\n",
    "$$\n",
    "D:=\\{w_{D1},w_{D2},\\ldots,W_{Dn}\\},\n",
    "$$\n",
    "where $W_{Dn}$ denotes the weight for word $n$ in document $D$. \n",
    "\n",
    "Examples of feature-extraction techniques are:\n",
    "1. Bag of Words model\n",
    "2. TF-IDF model\n",
    "\n",
    "We will use of the `nltk`, `gensim`, and `scikit-learn` libraries,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words model\n",
    "\n",
    "Each document is converted into a vector that represents the frequency of all the distinct words that are present in the document vector space for that specific document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Model\n",
    "\n",
    "The Bag of Words model is good, but the vectors are completely based on absolute frequencies of word occurrences. This has some potential problems where words that may tend to occur a lot across all documents in the corpus will have higher frequencies and will tend to overshadow other words that may not occur as frequently but may be more interesting and effective as features to identify specific categories for the documents. \n",
    "\n",
    "TF-IDF stands for *Term Frequency-Inverse Document Frequency*, a combination of two metrics:\n",
    "\n",
    "(i) Term frequency, $tf$, is what we had computed in the Bag of Words model (i.e., raw frequency value of that term in a particular document).\n",
    "\n",
    "(ii) Inverse document frequency, $idf$, is the inverse of the document frequency for each term. It is computed by dividing the total number of documents in our corpus by the document frequency for each term and then applying logarithmic scaling on the result.\n",
    "$$\n",
    "idf_{ij}=1+\\log \\frac{C}{1+df_{i}}\n",
    "$$\n",
    "where $idf_{ij}$ represents the $idf$ for the $j$-th term in document $i$, $C$ represents the count of the total number of documents in our corpus, and $df_{i}$ represents the frequency of the number of documents in which the term $i$ is present.\n",
    "\n",
    "- The final TF-IDF metric we will be using is a normalized version of the tfidf matrix we get from the product of \n",
    "$$\n",
    "T_{ij}:=tf_{ij}\\times idf_{ij}.\n",
    "$$\n",
    "\n",
    "- Typically, the TF-IDF matrix, \n",
    "$$\n",
    "T:=\\{T_{1},\\ldots,T_{n}\\}\n",
    "$$\n",
    "is normalized by dividing each $T_{i}$ with respect to the $L_2$ norm of the matrix (defined as the square root of the sum of the square of each $T_{i}$). The final $tfidf$ feature vector is given by:\n",
    "$$\n",
    "T_{i}:= \\frac{T_{i}}{||T_{i}||_{2}^{2}}, i=1,\\ldots,n.\n",
    "$$\n",
    "where $||T_{i}||_{2}^{2}$ represents the Euclidean norm for the TF-IDF matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "function gdrive_download () {\n",
    "  CONFIRM=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \"https://drive.google.com/uc?export=download&id=$1\" -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\n",
    "  wget --load-cookies /tmp/cookies.txt \"https://drive.google.com/uc?export=download&confirm=$CONFIRM&id=$1\" -O $2\n",
    "  rm -rf /tmp/cookies.txt\n",
    "}\n",
    "\n",
    "gdrive_download 1GGc42WEuqiiGtE8Hg6KTxO8QTBQqHNtS review_corpus_norm.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pickle\n",
    "review_corpus_norm = pickle.load(open('review_corpus_norm.pkl', \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF \n",
    "import numpy as np\n",
    "from feature_extractors import tfidf_transformer\n",
    "from feature_extractors import bow_extractor    \n",
    "\n",
    "def tf_idf(corpus):\n",
    "    # Bag of words construction\n",
    "    bow_vectorizer, bow_features = bow_extractor(corpus=corpus)\n",
    "    # feature names\n",
    "    feature_names = bow_vectorizer.get_feature_names()\n",
    "    # TF-IDF    \n",
    "    tfidf_trans, tdidf_features = tfidf_transformer(bow_features)\n",
    "    tdidf_features = np.round(tdidf_features.todense(),2)\n",
    "    return((tdidf_features, feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdidf_features,feature_names = tf_idf(review_corpus_norm) # memory error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is not enought memory to process all selected review, let's restrict our attention to business from *California*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_state(business,state = \"CA\"):\n",
    "    flag = any([category in state for category in business['state']])\n",
    "    return(flag)\n",
    "\n",
    "# get business_id\n",
    "business_id_ca = [business['business_id'] for business in train_business \\\n",
    "       if check_state(business,\"CA\")]\n",
    "\n",
    "# check if bussiness is in CA\n",
    "reviews_ca = [review['business_id'] in business_id_ca \\\n",
    "              for review in train_review] \n",
    "\n",
    "# subset reviews\n",
    "review_corpus_norm_ca = [review for (review,cond) in \\\n",
    "                         zip(review_corpus_norm,reviews_ca) if cond] \n",
    "\n",
    "print(\"No.reviews in CA:\",len(review_corpus_norm_ca),\"\\n\"\n",
    "      \"No.reviews in total:\",len(review_corpus_norm))\n",
    "\n",
    "tdidf_features,feature_names = tf_idf(review_corpus_norm_ca) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "*Topic models* have been designed specifically for the purpose of extracting various distinguishing concepts or topics from a large corpus containing various types of documents.\n",
    "\n",
    "Topic modeling is a *unsupervised* learning technique since involves extracting features from document terms to generate clusters or groups of terms that are distinguishable from each other, and these cluster of words form topics or concepts. \n",
    "\n",
    "Some methods for topic discovery include:\n",
    "- Latent semantic indexing:  \n",
    "    Singular value decomposition or principal components analysis\n",
    "- Latent Dirichlet allocation:  \n",
    "    Assumes a document is a mixture of a small number of topics\n",
    "- Non-negative matrix factorization\n",
    "\n",
    "Scikit-learn has examples: http://scikit-learn.org/0.18/auto_examples/applications/topics_extraction_with_nmf_lda.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tdidf_features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape # 11367 words, 4012 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non-negative Matrix Factorization\n",
    "def non_negative_marix_decomp(n_components,train_data):\n",
    "    import sklearn.decomposition as skld\n",
    "    model = skld.NMF(n_components=n_components, \n",
    "                     init='nndsvda', max_iter=500, \n",
    "                     random_state=0)\n",
    "    W = model.fit_transform(train_data)\n",
    "    H = model.components_\n",
    "    nmf = (W,H)\n",
    "    return(nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a long time\n",
    "r = 10 # no. of topics\n",
    "W_topic10,H_topic10 = \\\n",
    "    non_negative_marix_decomp(n_components = r, train_data = X) \n",
    "\n",
    "H_topic10 /= H_topic10.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/dipanjanS/text-analytics-with-python/master/Chapter-5/topic_modeling.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "num_topics = 10\n",
    "word_topic = W_topic10\n",
    "fontsize_base = 15# / np.max(word_topic) # font size for word with largest share in corpus\n",
    "\n",
    "for t in range(0, num_topics):\n",
    "    top_words_idx = np.argsort(word_topic[:,t])[::-1]  # descending order\n",
    "    top_words_idx = top_words_idx[:num_top_words]\n",
    "    top_words = [feature_names[k] for k in top_words_idx]\n",
    "    top_words_shares = word_topic[top_words_idx, t]\n",
    "    print('# Topic', t)\n",
    "    for i, (word, share) in enumerate(zip(top_words, top_words_shares)):\n",
    "        print(word, share)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 246\n",
    "r = 5 # no. of topics\n",
    "W_topic5,H_topic5 = \\\n",
    "    non_negative_marix_decomp(n_components = r, train_data = X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "num_topics = 5\n",
    "word_topic = W_topic5\n",
    "fontsize_base = 15# / np.max(word_topic) # font size for word with largest share in corpus\n",
    "\n",
    "for t in range(num_topics):\n",
    "    top_words_idx = np.argsort(word_topic[:,t])[::-1]  # descending order\n",
    "    top_words_idx = top_words_idx[:num_top_words]\n",
    "    top_words = [feature_names[k] for k in top_words_idx]\n",
    "    top_words_shares = word_topic[top_words_idx, t]\n",
    "    print('### Topic', t)\n",
    "    for i, (word, share) in enumerate(zip(top_words, top_words_shares)):\n",
    "        print(word, share)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
