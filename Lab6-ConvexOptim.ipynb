{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab6: Convex Optimization \n",
    "\n",
    "Outline:\n",
    "1. Convex optimization problem.\n",
    "2. Why convex optimization.\n",
    "3. Examples\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `cvxpy` for numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convex optimization problem\n",
    "\\begin{align*}\n",
    "    \\mbox{minimize} & \\ f_{0}(x) \\\\\n",
    "    \\mbox{subject to} & \\ f_{i}(x)\\leq 0, i=1,\\ldots,m \\\\\n",
    "                      & Ax = b\n",
    "\\end{align*}\n",
    "\n",
    "with variable $x\\in \\mathbb{R}^{n}$\n",
    "- Objective and inequality constraints $f_{0},\\ldots,f_{m}$ are convex; i.e., graphs of $f_{i}$ curve upward.\n",
    "- Equality constraints are *linear*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why convex optimization?\n",
    "\n",
    "- Beautiful, fairly complete, and useful theory.\n",
    "- Solution algorithms that work well in theory and practice.\n",
    "- many applications in:\n",
    "    + [machine learning](http://nbviewer.jupyter.org/github/cvxgrp/cvx_short_course/blob/master/intro/SVM.ipynb), [statistics](http://nbviewer.jupyter.org/github/cvxgrp/cvx_short_course/blob/master/applications/quantile_regression.ipynb)\n",
    "    + [control](http://nbviewer.jupyter.org/github/cvxgrp/cvx_short_course/blob/master/intro/control.ipynb)\n",
    "    + [signal processing](http://nbviewer.jupyter.org/github/cvxgrp/cvxpy/blob/master/examples/notebooks/WWW/robust_kalman.ipynb)\n",
    "    + [finance](http://nbviewer.jupyter.org/github/cvxgrp/cvx_short_course/blob/master/applications/portfolio_optimization.ipynb)\n",
    "    \n",
    "Very complete reference of convex optimization: https://web.stanford.edu/~boyd/cvxbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Examples\n",
    "\n",
    "## 3.1 Basic Example: Maximum Likelihood Estimation\n",
    "\n",
    "Suppose we have $n$ IDD realizations, $X_{1},\\ldots,X_{n}$, of a Poisson random variable with rate $\\lambda>0$ with pmf:\n",
    "$$\n",
    "p(x_{i}):= e^{-\\lambda} \\frac{\\lambda^{x_{i}}}{x_{i}!}, \\ \\lambda>0, \\ x_{i}=0,1,2,\\ldots\n",
    "$$\n",
    "\n",
    "The log-likelihood function in $\\lambda$ is thus given by:\n",
    "\\begin{align*}\n",
    "l(\\lambda) &:= \\log \\prod_{i=1}^{n} p(x_{i}) \\\\\n",
    "&= -n\\lambda + \\log\\lambda \\sum_{i=1}^{n}x_{i} - \\sum_{i=1}^{n}\\log(x_{i}!)\n",
    "\\end{align*}\n",
    "\n",
    "In order to find the MLE of $\\lambda$ we need to solve the following contraint optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mbox{maximize}   & l(\\lambda) \\\\\n",
    "\\mbox{subject to} & \\lambda >0 \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "n = 100\n",
    "lambda_true = 2\n",
    "x = np.random.poisson(lam=lambda_true,size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We start by declaring out the optimization [variable](http://www.cvxpy.org/en/latest/tutorial/intro/index.html#vectors-and-matrices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimization variable.\n",
    "lambda_ = cvx.Variable() # lambda is a python-specific variable so we use lambda_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We then define the the set of [contraints](http://www.cvxpy.org/en/latest/tutorial/intro/index.html#constraints) that apply to our optimization problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = [lambda_>=0] # Strict inequalities are not allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We construct the objective function (i.e., function to be optimized) using the admisible set of `cvx` [functions](http://www.cvxpy.org/en/latest/tutorial/functions/index.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = cvx.Maximize(-n*lambda_ + \\\n",
    "                   np.sum(x)*cvx.log(lambda_) - \\\n",
    "                   np.sum(np.log(sp.special.factorial(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Form and solve the optimization problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = cvx.Problem(obj, constraints)\n",
    "prob.solve()  # Returns the optimal VALUE (i.e., the value of the objective at the maximum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now obtain the actual maximizer of the log-likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_.value # which is close to the true value  lambda=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Nonnegative matrix factorization as a convex optimization problem\n",
    "\n",
    "A derivative work by Judson Wilson, 6/2/2014.    \n",
    "Adapted from the CVX example of the same name, by Argyris Zymnis, Joelle Skaf and Stephen Boyd\n",
    "\n",
    "Source: https://github.com/cvxgrp/cvxpy/blob/master/examples/notebooks/WWW/nonneg_matrix_fact.ipynb\n",
    "\n",
    "### Introduction\n",
    "\n",
    "We are given a matrix $X \\in \\mathbf{\\mbox{R}}^{v \\times n}$ and are interested in solving the problem:\n",
    "    \\begin{array}{ll}\n",
    "    \\mbox{minimize}   & \\| X - WH \\|_F \\\\\n",
    "    \\mbox{subject to} & W \\succeq 0 \\\\\n",
    "                      & H \\succeq 0,\n",
    "    \\end{array}\n",
    "where $W \\in \\mathbf{\\mbox{R}}^{v \\times r}$ and $H \\in \\mathbf{\\mbox{R}}^{r \\times n}$.\n",
    "\n",
    "This example generates a random matrix $X$ and obtains an\n",
    "*approximate* solution to the above problem by first generating\n",
    "a random initial guess for $W$ and then alternatively minimizing\n",
    "over $W$ and $H$ for a fixed number of iterations.\n",
    "\n",
    "### Generate problem data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "import numpy as np\n",
    "\n",
    "# Ensure repeatably random problem data.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate random data matrix X.\n",
    "v = 10 # e.g. number of frequency cells in our NBA example\n",
    "n = 10 # e.g. number of players\n",
    "r = 5  # e.g. number of bases vectors\n",
    "X = np.random.rand(v, r).dot(np.random.rand(r, n))\n",
    "\n",
    "# Initialize Y randomly.\n",
    "W_init = np.random.rand(v, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform alternating minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure same initial random Y, rather than generate new one\n",
    "# when executing this cell.\n",
    "W = W_init \n",
    "\n",
    "# Perform alternating minimization.\n",
    "MAX_ITERS = 30\n",
    "residual = np.zeros(MAX_ITERS)\n",
    "\n",
    "for iter_num in range(1, 1+MAX_ITERS):\n",
    "    # At the beginning of an iteration, X and Y are NumPy\n",
    "    # array types, NOT CVXPY variables.\n",
    "\n",
    "    # For odd iterations, treat Y constant, optimize over X.\n",
    "    if iter_num % 2 == 1:\n",
    "        H = cvx.Variable(r, n)\n",
    "        constraint = [H >= 0]\n",
    "        \n",
    "    # For even iterations, treat X constant, optimize over Y.\n",
    "    else:\n",
    "        W = cvx.Variable(v, r)\n",
    "        constraint = [W >= 0]\n",
    "    \n",
    "    # Solve the problem.\n",
    "    obj = cvx.Minimize(cvx.norm(X - W*H, 'fro'))\n",
    "    prob = cvx.Problem(obj, constraint)\n",
    "    prob.solve(solver=cvx.SCS)\n",
    "\n",
    "    if prob.status != cvx.OPTIMAL:\n",
    "        raise Exception(\"Solver did not converge!\")\n",
    "    \n",
    "    print('Iteration {}, residual norm {}'.format(iter_num, prob.value))\n",
    "    residual[iter_num-1] = prob.value\n",
    "\n",
    "    # Convert variable to NumPy array constant for next iteration.\n",
    "    if iter_num % 2 == 1:\n",
    "        H = H.value\n",
    "    else:\n",
    "        W = W.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to recover a sparse vector $\\beta\\in \\mathbb{R}^{n}$ from measurements $y\\in \\mathbb{R}^{m}$. Our measurement model tells us that \n",
    "$$\n",
    "y=X\\beta+\\epsilon,\n",
    "$$\n",
    "where $X\\in\\mathbb{R}^{m\\times n}$ is a known design matrix and $\\epsilon \\in \\mathbb{R}^{m}$ is unknown measurement error. \n",
    "\n",
    "The entries of $\\epsilon$ are drawn IID from the distribution $\\mathcal{N}(0,\\sigma^{2})$.\n",
    "\n",
    "We can first try to recover $x$ by solving the optimization problem\n",
    "$$\n",
    "\\mbox{minimize} \\ ||y-X\\beta||^{2}_{2}+\\gamma||\\beta||^{2}_{2}\n",
    "$$\n",
    "\n",
    "This problem is called *ridge* regression.\n",
    "\n",
    "The code below defines $n$, $m$, $X$, $\\beta$, and $y$. \n",
    "\n",
    "We use CVXPY to estimate $\\beta$ from $y$ using ridge regression for a given value of $\\gamma$, where we moreover try multiple values of $\\gamma$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy  as cvx\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Problem data.\n",
    "n = 15\n",
    "m = 10\n",
    "numpy.random.seed(1)\n",
    "X = numpy.random.randn(n, m)\n",
    "y = numpy.random.randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can define arbitrary [parameters](http://www.cvxpy.org/en/latest/tutorial/intro/index.html#parameters) to be part of our objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma must be positive due to DCP rules.\n",
    "gamma = cvx.Parameter(sign=\"positive\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are symbolic representations of constants. The purpose of parameters is to change the value of a constant in a problem without reconstructing the entire problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the problem.\n",
    "beta_ = cvx.Variable(m) # independent variable\n",
    "sum_of_squares = sum(cvx.square(X*beta_ - y))\n",
    "obj = cvx.Minimize(sum_of_squares + gamma*cvx.norm(beta_,2))\n",
    "prob = cvx.Problem(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma.value = 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.solve() # objective evaluated at solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_.value # estimated parameters for gamma = 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now repeat same procedure for several values of gamma,\n",
    "# in practice you would use cross-valudation to obtain the optimal value for gamma:\n",
    "gamma_vals = numpy.logspace(-4, 6) \n",
    "gamma_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below may take a while to excecute but we can visualize the resulting image here:\n",
    "http://www.cvxpy.org/en/latest/tutorial/intro/index.html#parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a trade-off curve of ||Ax-b||^2 vs. ||x||_1\n",
    "sq_penalty = []\n",
    "l2_penalty = []\n",
    "beta_values = []\n",
    "gamma_vals = numpy.logspace(-4, 6)\n",
    "for val in gamma_vals:\n",
    "    gamma.value = val\n",
    "    prob.solve()\n",
    "    # Use expr.value to get the numerical value of\n",
    "    # an expression in the problem.\n",
    "    sq_penalty.append(sum_of_squares.value)\n",
    "    l2_penalty.append(cvx.norm(beta_, 1).value)\n",
    "    beta_values.append(beta_.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,10))\n",
    "\n",
    "# Plot trade-off curve.\n",
    "plt.subplot(211)\n",
    "plt.plot(l2_penalty, sq_penalty)\n",
    "plt.xlabel(r'$\\||\\beta\\||_2$', fontsize=16) # we can include latex notation in labels\n",
    "plt.ylabel(r'$\\||y- X\\beta\\||^{2}_{2}$', fontsize=16)\n",
    "plt.title('Trade-Off Curve for Ridge Regression', fontsize=16)\n",
    "\n",
    "# Plot entries of beta vs. gamma.\n",
    "plt.subplot(212)\n",
    "for i in range(m):\n",
    "    plt.plot(gamma_vals, [xi[i,0] for xi in beta_values])\n",
    "plt.xlabel(r'$\\gamma$', fontsize=16)\n",
    "plt.ylabel(r'$\\beta_{i}$', fontsize=16)\n",
    "plt.xscale('log')\n",
    "plt.title(r'Entries of $\\beta$ vs. $\\gamma$', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
